# Scale drives machine learning progress

->

# Quy Mô Quyết Định Tiến Trình Phát Triển Machine Learning


Many of the ideas of deep learning (neural networks) have been around for decades. Why are these ideas taking off now?

->

Rất nhiều những ý tưởng của học sâu (mạng neural) đã xuất hiện từ hàng thập kỷ trước.  Vậy tại sao tận bây giờ chúng mới cất cánh, mới bùng nổ như vậy?  




Two of the biggest drivers of recent progress have been:

->

Hai nguyên nhân mang tính quyết định tới những phát triển gần đây là


* **Data availability.​** People are now spending more time on digital devices (laptops, mobile devices). Their digital activities generate huge amounts of data that we can feed to our learning algorithms.

->

**Dữ liệu sẵn có**. Ngày nay,  mọi người dành nhiều thời gian hơn bên những thiết bị số như máy tính xách tay, thiết bị di động, .v.v. Chính những thiết bị số này là nguồn tạo ra lượng dữ liệu cực lớn - dữ liệu vẫn dùng cho những thuật toán học máy ngày nay.  




* **Computational scale.** ​We started just a few years ago to be able to train neural networks that are big enough to take advantage of the huge datasets we now have.

-> 

**Quy mô năng lực tính toán**. Mãi mấy năm gần đây chúng ta mới có thể huấn luyện mạng neural đủ lớn để tận dụng được lượng dữ liệu big data. 

 


In detail, even as you accumulate more data, usually the performance of older learning algorithms, such as logistic regression, “plateaus.” This means its learning curve “flattens out,” and the algorithm stops improving even as you give it more data:

->

Cho dù có thêm nhiều nhiều dữ liệu nữa, thì hiệu quả của thuật toán học máy thế hệ cũ như hồi quy logistic cũng chẳng mảy may cải thiện. Nghĩa là đồ thị chất lượng học nằm ngang và thuật toán sẽ không cải thiện chất lượng cho dù thêm bao nhiêu dữ liệu. 


![img](../imgs/C04_01.png)


It was as if the older algorithms didn’t know what to do with all the data we now have.

->

Như thể thuật toán cổ điển không biết xử lý ra sao với tất cả lượng dữ liệu đang có. 




If you train a small neutral network (NN) on the same supervised learning task, you might get slightly better performance:

->

Nếu bạn huấn luyện một mạng neural nhỏ cho cùng một tác vụ học có kiểm soát, có thể bạn sẽ đạt chất lượng cao hơn một chút


![img](../imgs/C04_02.png)

Here, by “Small NN” we mean a neural network with only a small number of hidden units/layers/parameters. Finally, if you train larger and larger neural networks, you can obtain even better performance [1]:

->

"Mạng neural nhỏ" ở đây có nghĩa là mạng neural với ít các nút ẩn/tầng/tham số. Và sau cùng, càng huấn luyện mạng neural to càng cho kết quả tốt hơn [1]: 



![img](../imgs/C04_03.png)

Thus, you obtain the best performance when you (i) Train a very large neural network, so that you are on the green curve above; (ii) Have a huge amount of data.

->

Vì thế bạn chỉ thu về chất lượng tốt nhất khi (i) huấn luyện mạng neural rất lớn tương ứng với đường chất lượng màu xanh trong hình vẽ và (ii) khi có lượng dữ liệu cực lớn. 



Many other details such as neural network architecture are also important, and there has been much innovation here. But one of the more reliable ways to improve an algorithm’s performance today is still to (i) train a bigger network and (ii) get more data.

->

Còn nhiều chi tết quan trọng khác nữa như kiến trúc mạng neural. Phần lớn những sáng tạo, công trình nghiên cứu đều nằm ở đấy. Nhưng cách đơn giản mà tin cậy để tăng chất lượng thuật toán vẫn là huấn luyện mạng to hơn trên dữ liệu lớn hơn. 



**FOOTNOTE:**

[1] This diagram shows NNs doing better in the regime of small datasets. This effect is less consistent than the effect of NNs doing well in the regime of huge datasets. In the small data regime, depending on how the features are hand-engineered, traditional algorithms may or may not do better. For example, if you have 20 training examples, it might not matter much whether you use logistic regression or a neural network; the hand-engineering of features will have a bigger effect than the choice of algorithm. But if you have 1 million examples, I would favor the neural network.

->

[1] Hình vẽ thể hiện mạng neural cho kết quả tốt hơn trong điều kiện dữ liệu nhỏ. Nhận định này ít đồng thuận hơn với nhận định mạng neural với dữ liệu lớn.  Đối với dữ liệu nhỏ, tùy thuộc vào cách tạo đặc trưng thủ công mà thuật toán truyền thống có thể hoặc không cho kết quả tốt hơn. Ví dụ như, nếu ta chỉ có 20 mẫu huấn luyện thì không đủ để dùng hồi suy logistic lẫn mạng neural. Nhưng đổi lại đặc trưng thủ công lại chịu ảnh hưởng nhiều từ thuật toán học máy. Nhưng nếu có 1 triệu mẫu thì tôi sẽ chọn mạng neural. 



The process of how to accomplish (i) and (ii) are surprisingly complex. This book will discuss the details at length. We will start with general strategies that are useful for both traditional learning algorithms and neural networks, and build up to the most modern strategies for building deep learning systems.

->

Bạn có thể bất ngờ với độ phức tạp của cả quá trình phát triển để đạt được điều (i) và (ii). Quyển sách này sẽ thảo luận chi tiết và sâu hơn. Chúng ta sẽ bắt đầu bằng những chiến thuật hữu dụng cho cả thuật toán truyền thống lẫn mạng neural, rồi từ từ xây dựng chiến thuật tân tiến nhất cho xây dựng mạng deep learning. 

